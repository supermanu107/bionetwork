{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "centrality.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRb1JoypnubmcIQoerjls3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supermanu107/bionetwork/blob/main/centrality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLG9urFbriKW",
        "outputId": "8cf99cba-de0b-4c53-8039-da054ef23468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter NCBI Taxonomy ID: 511145\n",
            "\n",
            "Downloading proteins from STRING database https://stringdb-static.org/download/protein.links.v11.5/511145.protein.links.v11.5.txt.gz\n",
            "[Errno 2] No such file or directory: 'data/netgenes_species_list.csv'\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import requests\n",
        "import shutil\n",
        "import re\n",
        "import os\n",
        "from urllib import request\n",
        "\n",
        "\n",
        "# Download protein links from STRING website\n",
        "def download_protein_url(url, protein_link_gz_file_name):\n",
        "    print(\"\\nDownloading proteins from STRING database\", url)\n",
        "    request.urlretrieve(url=url, filename=protein_link_gz_file_name)\n",
        "    protein_link_file_name = re.split(pattern=r'\\.gz', string=protein_link_gz_file_name)[0]\n",
        "\n",
        "    # Unzip \".gz\" file and save in data folder\n",
        "    with gzip.open(protein_link_gz_file_name, 'rb') as f_in:\n",
        "        with open(protein_link_file_name, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    # delete the *.gz file\n",
        "    if os.path.exists(protein_link_gz_file_name):\n",
        "        os.remove(protein_link_gz_file_name)\n",
        "\n",
        "\n",
        "# Download essential genes from netgenes website\n",
        "def download_netgenes_url(url, organism_name):\n",
        "    print(\"Downloading essential genes from Netgenes database: \", url)\n",
        "    with open(organism_name + '.csv', \"wb\") as file:\n",
        "        response = requests.get(url)\n",
        "        file.write(response.content)\n",
        "\n",
        "\n",
        "try:\n",
        "    taxonomyID = int(input(\"Enter NCBI Taxonomy ID: \"))\n",
        "\n",
        "    # Download all protein connection links from STRING database\n",
        "    protein_link_file_name = str(taxonomyID) + \".protein.links.v11.5.txt.gz\";\n",
        "    protein_url = \"https://stringdb-static.org/download/protein.links.v11.5/\" + protein_link_file_name\n",
        "    download_protein_url(protein_url, protein_link_file_name)\n",
        "\n",
        "    # Select organism name matching taxonomy id\n",
        "    df_netgenes_species = pd.read_csv(\"data/netgenes_species_list.csv\", sep=',')\n",
        "    df_netgenes_selected = df_netgenes_species.loc[df_netgenes_species['NCBI Taxonomy ID'] == taxonomyID]\n",
        "    selected_organism_name = df_netgenes_selected['Organism Name'].values[0]\n",
        "\n",
        "    df_all = pd.read_csv(str(taxonomyID) + '.protein.links.v11.5.txt', sep=' ')\n",
        "    # filter  > 700 combined_score\n",
        "    df_filtered = df_all[df_all['combined_score'] >= 700]\n",
        "    df_data = df_filtered[['protein1', 'protein2', 'combined_score']]\n",
        "    G1 = nx.from_pandas_edgelist(df_data, \"protein1\", \"protein2\", [\"combined_score\"])\n",
        "    num_of_nodes = G1.number_of_nodes()\n",
        "    num_of_edges = G1.number_of_edges()\n",
        "\n",
        "    # read essential genes from NetGenes Data\n",
        "    netgenes_url = \"https://rbc-dsai-iitm.github.io/NetGenes/CSV/\" + urllib.parse.quote(selected_organism_name) + \".csv\"\n",
        "    download_netgenes_url(netgenes_url, selected_organism_name)\n",
        "    df_essential_genes_all = pd.read_csv('data/' + selected_organism_name + '.csv', header=0,\n",
        "                                         names=['node', 'alias', 'notes', 'score'], sep=',')\n",
        "    num_of_essential_nodes = df_essential_genes_all.shape[0]\n",
        "\n",
        "\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', 200)\n",
        "\n",
        "    essential_nodes_ratio = \"{:.1f} %\".format((num_of_essential_nodes/num_of_nodes)*100)\n",
        "    table1_dict = {\n",
        "        \"Organism (NCBI taxonomy ID)\": [selected_organism_name + \" (\" + str(taxonomyID) + \")\"],\n",
        "        \"Nodes (proteins)\" : [num_of_nodes],\n",
        "        \"Essential nodes\": [num_of_essential_nodes],\n",
        "        \"(%)\": [essential_nodes_ratio],\n",
        "        \"Edges (interactions)\": [num_of_edges]\n",
        "            }\n",
        "\n",
        "    print(\"\\nTable 1 - Summary of the networks\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    df_table1 = pd.DataFrame(table1_dict)\n",
        "    print(df_table1)\n",
        "\n",
        "    df_essential_gene_data_list = df_essential_genes_all['node'].values.tolist()\n",
        "\n",
        "    # match protein names against essential genes and find match\n",
        "    # Essential to Essential (EE)\n",
        "    df_ee_data = df_data.loc[df_data['protein1'].isin(df_essential_gene_data_list) & df_data['protein2'].isin(df_essential_gene_data_list)]\n",
        "    Gee = nx.from_pandas_edgelist(df_ee_data, \"protein1\", \"protein2\", [\"combined_score\"])\n",
        "    num_ee = Gee.number_of_edges()\n",
        "    num_ee_ratio = \"{:.3f}\".format((num_ee/num_of_edges))\n",
        "\n",
        "    # Non-Essential to Essential (NE)\n",
        "    df_ne_data = df_data.loc[-df_data['protein1'].isin(df_essential_gene_data_list) & df_data['protein2'].isin(df_essential_gene_data_list)]\n",
        "    Gne = nx.from_pandas_edgelist(df_ne_data, \"protein1\", \"protein2\", [\"combined_score\"])\n",
        "    num_ne = Gne.number_of_edges()\n",
        "    num_ne_ratio = \"{:.3f}\".format((num_ne/num_of_edges))\n",
        "\n",
        "    # Non-Essential to Non-Essential (NN)\n",
        "    df_nn_data = df_data.loc[~df_data['protein1'].isin(df_essential_gene_data_list) & ~df_data['protein2'].isin(df_essential_gene_data_list)]\n",
        "    Gnn = nx.from_pandas_edgelist(df_nn_data, \"protein1\", \"protein2\", [\"combined_score\"])\n",
        "    num_nn = Gnn.number_of_edges()\n",
        "    num_nn_ratio = \"{:.3f}\".format((num_nn/num_of_edges))\n",
        "\n",
        "    assortativity_coefficient = nx.degree_assortativity_coefficient(G1)\n",
        "\n",
        "    print(\"\\nTable 2 - Assortativity coefficients for the networks\")\n",
        "    print(\"--------------------------------------------------------------------------------\")\n",
        "    table2_dict = {\n",
        "        \"Organism\": [selected_organism_name],\n",
        "        \"Nodes (proteins)\": [num_of_nodes],\n",
        "        \"EE\" : [num_ee_ratio],\n",
        "        \"NE\": [num_ne_ratio],\n",
        "        \"NN\": [num_nn_ratio],\n",
        "        \"Total edges\": [num_of_edges],\n",
        "        \"Assortavity (r)\": assortativity_coefficient\n",
        "            }\n",
        "    df_table2 = pd.DataFrame(table2_dict)\n",
        "    print(df_table2)\n",
        "\n",
        "    # Centrality measures for EE data\n",
        "    G = nx.from_pandas_edgelist(df_ee_data, \"protein1\", \"protein2\", [\"combined_score\"])\n",
        "\n",
        "    # \"https://stackoverflow.com/questions/50243215/networkx-how-to-write-to-csv-multiple-centrality-metrics\"\n",
        "    df_centrality_measures = pd.DataFrame(dict(\n",
        "        DEGREE_CENTRALITY=nx.degree_centrality(G),\n",
        "        EIGENVECTOR=nx.eigenvector_centrality(G),\n",
        "        CLOSENESS_CENTRALITY=nx.closeness_centrality(G),\n",
        "        BETWEENNESS_CENTRALITY=nx.betweenness_centrality(G)\n",
        "    ))\n",
        "    \n",
        "    #\n",
        "    output_file_name = str(taxonomyID) + '-centrality.csv'\n",
        "    print(\"\\nSaving all centrality values in {}\".format(output_file_name))\n",
        "    df_centrality_measures.to_csv(output_file_name, index_label='Essential Node')\n",
        "\n",
        "    print('\\nDone')\n",
        "\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ]
    }
  ]
}